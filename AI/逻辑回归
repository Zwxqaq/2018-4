###逻辑回归：
  其实是处理分类任务，不是回归算法，最经典的二分类算法，运用范围广
  xita参数越大，则在分类过程中越重要
  可解决多分类问题---》softmax
  算法选择原则：先用简单再用复杂
  决策边界：可以是非线性的
###sigmoid：
   g（z）=1/（1+e^-z) 
   z=线性回归的输入 xita×x  多少个参数 --》多少个xita
   值域：[0,1]
   自变量：任意实数
   将输入映射到[0,1]之间---》由值到概率的转换----》线性回归得到一个预测值，再将该值映射到sigmoid函数，完成分类任务
   ###二分类
   预测函数h（x）
     P（y=0|x;xita）=h（x）
     P（y=1|x;xita）=1-h（x）
     整合：p（y|x，xita）=h（x）^y * (1-h(x))^(1-y)
###步长×方向=更新的数值
  参数更新：更新的xita=原xita - 1/（mini_batch)×更新的数值
  似然函数----》对数似然----》引入J（xita）=-（1/m）× l（xita）转换梯度下降任务--》求偏导，得到结果值--》更新参数
