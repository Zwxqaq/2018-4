线性回归J（xita）---》目标函数  偏导求xita最小值 最小二分法
###梯度：偏导数 
    ###梯度下降：下山方向，梯度上升反方向
      不断求解：直到最优解----》一次一次迭代，不断优化
      目标函数J（xita）----》求各个xita方向到偏导
    ###什么参数让目标函数达到极值点----》寻找随机点，找最合适到方向，步伐小，不一定达到收敛和最终结果状态
                  ----》通过方向和步伐去更新权重参数
    ###批量梯度下降：
      损失函数==目标函数，综合考虑所有样本，准确率高，容易得到最优解，速度很慢
    ###随机梯度下降：
      每次找一个样本，比批量少的多，寻找速度快，样本的好坏不知，噪音点/异常点，使得最终效果不一定好，因为更新的权重参数不一定好
      每次到收敛都不一样
    ###小批量梯度下降：
      mini -batch小批量：每次选择一小部分样本数据，然后不断更新权重参数
      例如先用10个函数构造损失函数（目标函数），然后进行不断偏导更新权重
      常见mini-batch ：32，64，128 考虑内存和效率  ---》迭代尽可能大一些，使得结果更稳定一些
    ###求偏导数，沿着偏导方向更新：
      更新（步长多大）----》又叫作学习率
      学习率过大，影响巨大，可能绕过最低点找到极值点
      学习率过小：迭代速度慢，但是可以不断优化----》小的学习率，大的迭代次数迭代
      learning rate：选择小到学习率，让目标函数（损失函数）收敛
      比如学习率：0.01 --->0.005---->0.001 迭代时不断减少
    ### 
      不同梯度下降算法----》对逻辑回归到影响   损失函数在不同xita的优化下逐步收敛  --》预测值不断接近真实值
    
